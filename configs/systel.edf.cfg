###
### To add MPI support
###
#
# options: mpi
#
# cmd_obj: ... -DHAVE_MPI ...
#
# libs_partel: ... <path_to_metis>/lib/libmetis.a ...
#
# Replace in cmd_obj and cmd_exec the compiler by the mpi_compiler
#
###
### TO ADD VTK SUPPORT
###
# cmd_obj: ... -DHAVE_VTK ...
#
###
### TO ADD MED SUPPORT
###
# cmd_obj: ... -DHAVE_MED ...
#
# incs_all: ... -I<path-to-med>/include ...
# libs_all: ... -L<path-to-hdf5>/lib -L<path-to-med>/lib -lmed -lhdf5 -lstdc++ -lz ...
###
### TO ADD DREGEDIM SUPPORT
###
# cmd_obj: ... -DHAVE_DREDGESIM
# modules = system
#
#  !!! You will need to regenerate the cmdf files
#  !!! First remove all the cmdf
#  !!! On linux run the following command from the root directory
#  !!! find sources/ -iname *.cmdf -exec rm -vf {} +
#  !!! Then run compileTELEMAC.py --rescan
#
###
### TO ADD SCOTCH SUPPORT
###
#  !!! SCOTCH support still needs metis
# cmd_obj: ... -DHAVE_SCOTCH ...
# libs_partel: ... -L<path-to-scotch>/lib -lsctoch lsctocherr <path-to-metis>/lib/libmetis.a ...
#
###
### TO ADD PARMETIS SUPPORT
###
# cmd_obj: ... -DHAVE_PARMETIS ...
# libs_partel: ... <path-to-parmetis>/lib/libparmetis.a ...
#
###
### TO ADD PTSCOTCH SUPPORT
###
#  !!! PTSCOTCH support still needs parmetis
# cmd_obj: ... -DHAVE_PARMETIS -DHAVE_PTSCOTCH ...
# libs_partel: ... -L<path-to-ptscotch>/lib -lptsctoch lptsctocherr <path-to-parmetis>/lib/libparmetis.a ...
#
# _____                              _______________________________
# ____/ TELEMAC Project Definitions /______________________________/
#
[Configurations]
configs: C9.gfortran.dyn C9.gfortran.debug gcov
         C9.nag C9.nag.debug
         C9.intel C9.intel.debug
         athos.intel athos.intel.debug
         porthos.intel porthos.intel.debug
         eole.intel.dyn eole.intel.debug
         winHPC winHPC.debug
# _____                        ____________________________________
# ____/ General /___________________________________/
# Global declaration that are true for all the C9.* configurations
[general]
language: 2
modules:  system
version:  trunk
#
sfx_zip:    .zip
sfx_lib:    .a
sfx_obj:    .o
sfx_mod:    .mod
sfx_exe:
#
#
val_root:   <root>/examples
#
val_rank:   all
#
mods_all:   -I <config>
#
incs_all: -I $MEDHOME/include -I $MUMPSHOME/include
libs_all: $MUMPSHOME/lib/libdmumps.a
          $MUMPSHOME/lib/libmumps_common.a
          $MUMPSHOME/lib/libpord.a
          $SCALAPACKHOME/lib/libscalapack.a
          $BLACSHOME/LIB/blacs_MPI-LINUX-0.a
          $BLACSHOME/LIB/blacsF77init_MPI-LINUX-0.a
          $BLACSHOME/LIB/blacs_MPI-LINUX-0.a
          -L/local/BLAS -lblas
          -lm -L$MEDHOME/lib -lmed -L$HDF5HOME/lib -lhdf5 -ldl -lstdc++ -lz
          -L$METISHOME/lib -lmetis
#
cmd_obj_c: gcc -c <srcName> -o <objName>
#
# _____          __________________________________
# ____/ Calibre9 _________________________________/
#
# Gfortran
#
[C9.gfortran.dyn]
brief: Gfortran compiler 4.9.2 with open_mpi for a debian 8 all libraries are compiled in dynamic
#
sfx_lib:    .so
#
mpi_cmdexec: mpirun -machinefile MPI_HOSTFILE -np <ncsize> <exename>
#
cmd_obj:    mpif90 -c -cpp -fPIC -O2 -fconvert=big-endian -frecord-marker=4 -DHAVE_MPI -DHAVE_MUMPS -DHAVE_MED -DHAVE_VTK <mods> <incs> <f95name>
cmd_lib:    mpif90 -fPIC -shared -fconvert=big-endian -frecord-marker=4 -lpthread -lm -o <libname> <objs>
cmd_exe:    mpif90 -fPIC -fconvert=big-endian -frecord-marker=4 -lpthread -lm -o <exename> <objs> <libs>
#
libs_all: -L$MUMPSHOME/lib -ldmumps -lmumps_common -lpord
          -L$SCALAPACKHOME/lib -lscalapack
          -lblas
          -lm -L$MEDHOME/lib -lmed -L$HDF5HOME/lib -lhdf5 -ldl -lstdc++ -lz
          -L$METISHOME/lib -lmetis

cmd_obj_c: gcc -c -fPIC <srcName> -o <objName>
#
#
[C9.gfortran.debug]
brief: Debug mode Gfortran compiler 4.9.2 with open_mpi for a debian 8
#
sfx_lib:    .so
#
mpi_cmdexec: mpirun -machinefile MPI_HOSTFILE -np <ncsize> <exename>
#
cmd_obj:    mpif90 -c -cpp -fPIC -O0 -g -Wall -fcheck=all -fbacktrace -ffpe-trap=invalid,zero,overflow -fconvert=big-endian -frecord-marker=4 -DHAVE_MPI -DHAVE_MUMPS -DHAVE_MED -DHAVE_VTK <mods> <incs> <f95name>
cmd_lib:    mpif90 -fPIC -shared -O0 -g -Wall -fcheck=all -fbacktrace -ffpe-trap=invalid,zero,overflow -fconvert=big-endian -frecord-marker=4 -lpthread -lm -o <libname> <objs>
cmd_exe:    mpif90 -fPIC -g -Wall -fcheck=all -fbacktrace -ffpe-trap=invalid,zero,overflow -fconvert=big-endian -frecord-marker=4 -lpthread -lm -o <exename> <objs> <libs>
#
libs_all: -L$MUMPSHOME/lib -ldmumps -lmumps_common -lpord
          -L$SCALAPACKHOME/lib -lscalapack
          -lblas
          -lm -L$MEDHOME/lib -lmed -L$HDF5HOME/lib -lhdf5 -ldl -lstdc++ -lz
          -L$METISHOME/lib -lmetis

cmd_obj_c: gcc -c -fPIC <srcName> -o <objName>
#
#
[gcov]
brief: Code coverage mode Gfortran compiler 4.9.2 with open_mpi for a debian 8
#
options:    mpi
#
mpi_cmdexec: mpirun -machinefile MPI_HOSTFILE -np <ncsize> <exename>
#
cmd_obj:    mpif90 -c -O2 -fprofile-arcs -ftest-coverage -fPIC -fconvert=big-endian -frecord-marker=4 -DHAVE_MPI -DHAVE_MED -DHAVE_VTK <mods> <incs> <f95name>
cmd_lib:    ar cru <libname> <objs>
cmd_exe:    mpif90 -fprofile-arcs -ftest-coverage -fPIC -fconvert=big-endian -frecord-marker=4 -lpthread -lm -o <exename> <objs> <libs>
#
# Intel
#
[C9.intel]
brief: Intel compiler 10.1 with mpich for a debian 8
#
options:    mpi
modules: system -mascaret
#
mpi_cmdexec: mpirun -machinefile MPI_HOSTFILE -np <ncsize> <exename>
#
cmd_obj:    mpif90 -c -O2 -convert big_endian -DHAVE_VTK -DNO_INQUIRE_SIZE -DHAVE_MED -DHAVE_MPI -DHAVE_MUMPS <mods> <incs> <f95name>
cmd_lib:    ar cru <libname> <objs>
cmd_exe:    mpif90 -convert big_endian -o <exename> <objs> <libs>
#
#
[C9.intel.debug]
brief: Debug mode Intel compiler 10.1 with mpich for a debian 8
#
options:    mpi
modules: system -mascaret
#
mpi_cmdexec: mpirun -machinefile MPI_HOSTFILE -np <ncsize> <exename>
#
cmd_obj:    mpif90 -c -O3 -convert big_endian -debug all -check all -traceback -DHAVE_VTK -DNO_INQUIRE_SIZE -DHAVE_MED -DHAVE_MPI <mods> <incs> <f95name>
cmd_lib:    ar cru <libname> <objs>
cmd_exe:    mpif90 -convert big_endian -o <exename> <objs> <libs>
#
# Nag
#
[C9.nag]
brief: Nag compiler 5.3.1 with mpich for a debian 8
#
options:    mpi
modules: system -mascaret
#
mpi_cmdexec: mpirun -machinefile MPI_HOSTFILE -np <ncsize> <exename>
#
cmd_obj:    mpif90 -c -O2 -w=obs -Oassumed -convert=BIG_ENDIAN -DNAGFOR -DHAVE_MED -DHAVE_MPI -DHAVE_MUMPS <mods> <incs> <f95name>
cmd_lib:    ar cru <libname> <objs>
cmd_exe:    mpif90 -o <exename> <objs> <libs>
#
#
[C9.nag.debug]
brief: Debug mode Nag compiler 5.3.1 with mpich for a debian 8
#
options:    mpi
modules: system -mascaret
#
mpi_cmdexec: mpirun -machinefile MPI_HOSTFILE -np <ncsize> <exename>
#
cmd_obj:    mpif90 -c -O0 -g -C=all -gline -w=obs -Oassumed -convert=BIG_ENDIAN -DHAVE_MED -DHAVE_MPI <mods> <incs> <f95name>
cmd_lib:    ar cru <libname> <objs>
cmd_exe:    mpif90 -g -gline -o <exename> <objs> <libs>
#
# _____                ____________________________________
# ____/ Athos cluster /___________________________________/
#
[athos.intel]
brief: Intel 16.0.4 compiler with open_mpi 1.6.5_tuned on the EDF athos cluster
#
options:    mpi hpc
par_cmdexec: srun -n 1 -N 1 <config>/partel < PARTEL.PAR >> <partel.log>
mpi_cmdexec: mpirun -np <ncsize> <exename>
#
hpc_stdin: #!/bin/bash
  #SBATCH --job-name=<jobname>
  #SBATCH --output=<jobname>-<time>.out
  #SBATCH --error=<jobname>-<time>.err
  #SBATCH --time=<walltime>
  #SBATCH --ntasks=<ncsize>
  #SBATCH --partition=<queue>
  ##SBATCH --exclude=cn[0000-0000,0000]
  #SBATCH --exclusive
  #SBATCH --nodes=<ncnode>
  #SBATCH --ntasks-per-node=<nctile>
  source <root>/configs/pysource.<configName>.sh
  <py_runcode>
#
hpc_runcode: cp HPC_STDIN ../;cd ../;sbatch < <hpc_stdin>
#
cmd_obj:    mpif90  -c -convert big_endian -O2 -DHAVE_MPI -DHAVE_MED -DHAVE_VTK <mods> <incs> <f95name>
cmd_lib:    ar cru <libname> <objs>
cmd_exe:    mpif90  -o <exename> <objs> <libs>
#
incs_all: -I $MEDHOME/include
libs_all: -lm -L$MEDHOME/lib -lmed -L$HDF5HOME/lib -lhdf5 -ldl -lstdc++ -lz
          -L$METISHOME/lib -lmetis
#
cmd_obj_c: gcc -c <srcName> -o <objName>
#
#
[athos.intel.debug]
brief: Debug mode Intel 16.0.4 compiler with open_mpi 1.6.5_tuned on the EDF athos cluster
#
options:    mpi hpc
par_cmdexec: srun -n 1 -N 1 <config>/partel < PARTEL.PAR >> <partel.log>
mpi_cmdexec: mpirun -np <ncsize> <exename>
#
hpc_stdin: #!/bin/bash
  #SBATCH --job-name=<jobname>
  #SBATCH --output=<jobname>-<time>.out
  #SBATCH --error=<jobname>-<time>.err
  #SBATCH --time=<walltime>
  #SBATCH --ntasks=<ncsize>
  #SBATCH --partition=<queue>
  ##SBATCH --exclude=cn[0000-0000,0000]
  #SBATCH --exclusive
  #SBATCH --nodes=<ncnode>
  #SBATCH --ntasks-per-node=<nctile>
  source <root>/configs/pysource.<configName>.sh
  <py_runcode>
#
hpc_runcode: cp HPC_STDIN ../;cd ../;sbatch < <hpc_stdin>
#
cmd_obj:    mpif90  -c -convert big_endian -debug all -check all -traceback -ftrapuv -O0 -g -DHAVE_MPI -DHAVE_MED -DHAVE_VTK <mods> <incs> <f95name>
cmd_lib:    ar cru <libname> <objs>
cmd_exe:    mpif90  -g -debug all -check all -traceback -ftrapuv -o <exename> <objs> <libs>
#
incs_all: -I $MEDHOME/include
libs_all: -lm -L$MEDHOME/lib -lmed -L$HDF5HOME/lib -lhdf5 -ldl -lstdc++ -lz
          -L$METISHOME/lib -lmetis
#
cmd_obj_c: gcc -c <srcName> -o <objName>
#
# _____                  ____________________________________
# ____/ Porthos cluster /___________________________________/
#
[porthos.intel]
brief: Intel compiler 15.1.133 with open_mpi 1.8.2rc6mlnx for porhos edf cluster
#
options:    mpi hpc
par_cmdexec: srun -n 1 -N 1 <config>/partel < PARTEL.PAR >> <partel.log>
mpi_cmdexec: mpirun -np <ncsize> <exename>
#
hpc_stdin: #!/bin/bash
  #SBATCH --job-name=<jobname>
  #SBATCH --output=<jobname>-<time>.out
  #SBATCH --error=<jobname>-<time>.err
  #SBATCH --time=<walltime>
  #SBATCH --ntasks=<ncsize>
  #SBATCH --partition=<queue>
  ##SBATCH --exclude=cn[0000-0000,0000]
  #SBATCH --exclusive
  #SBATCH --nodes=<ncnode>
  #SBATCH --ntasks-per-node=<nctile>
  source <root>/configs/pysource.<configName>.sh
  <py_runcode>
#
hpc_runcode: cp HPC_STDIN ../;cd ..;sbatch < <hpc_stdin>
#
cmd_obj:    mpif90  -c -convert big_endian -O2 -DHAVE_MPI -DHAVE_MED -DHAVE_VTK <mods> <incs> <f95name>
cmd_lib:    ar cru <libname> <objs>
cmd_exe:    mpif90  -o <exename> <objs> <libs>
#
incs_all: -I $MEDHOME/include
libs_all: -lm -L$MEDHOME/lib -lmed -L$HDF5HOME/lib -lhdf5 -ldl -lstdc++ -lz
          -L$METISHOME/lib -lmetis
#
#
[porthos.intel.debug]
brief: Debug mode Intel compiler 15.1.133 with open_mpi 1.8.2rc6mlnx for porhos edf cluster
#
options:    mpi hpc
par_cmdexec: srun -n 1 -N 1 <config>/partel < PARTEL.PAR >> <partel.log>
mpi_cmdexec: mpirun -np <ncsize> <exename>
#
hpc_stdin: #!/bin/bash
  #SBATCH --job-name=<jobname>
  #SBATCH --output=<jobname>-<time>.out
  #SBATCH --error=<jobname>-<time>.err
  #SBATCH --time=<walltime>
  #SBATCH --ntasks=<ncsize>
  #SBATCH --partition=<queue>
  ##SBATCH --exclude=cn[0000-0000,0000]
  #SBATCH --exclusive
  #SBATCH --nodes=<ncnode>
  #SBATCH --ntasks-per-node=<nctile>
  source <root>/configs/pysource.<configName>.sh
  <py_runcode>
#
hpc_runcode: cp HPC_STDIN ../;cd ..;sbatch < <hpc_stdin>
#
cmd_obj:    mpif90  -c -convert big_endian -debug all -check all -traceback -ftrapuv -O0 -g -DHAVE_MPI -DHAVE_MED -DHAVE_VTK <mods> <incs> <f95name>
cmd_lib:    ar cru <libname> <objs>
cmd_exe:    mpif90  -g -debug all -check all -traceback -ftrapuv -o <exename> <objs> <libs>
#
incs_all: -I $MEDHOME/include
libs_all: -lm -L$MEDHOME/lib -lmed -L$HDF5HOME/lib -lhdf5 -ldl -lstdc++ -lz
          -L$METISHOME/lib -lmetis
#
# _____               ____________________________________
# ____/ Eole cluster /___________________________________/
#
[eole.intel.dyn]
brief: Intel compiler 2017 with intel mpi 2017.0.998 in dynamic
#
sfx_lib:    .so
options:    mpi hpc
par_cmdexec: srun -n 1 -N 1 <config>/partel < PARTEL.PAR >> <partel.log>
mpi_cmdexec: mpirun -np <ncsize> <exename>
#
hpc_stdin: #!/bin/bash
  #SBATCH --job-name=<jobname>
  #SBATCH --output=<jobname>-<time>.out
  #SBATCH --error=<jobname>-<time>.err
  #SBATCH --time=<walltime>
  #SBATCH --ntasks=<ncsize>
  #SBATCH --partition=<queue>
  ##SBATCH --exclude=cn[0000-0000,0000]
  #SBATCH --exclusive
  #SBATCH --nodes=<ncnode>
  #SBATCH --ntasks-per-node=<nctile>
  source <root>/configs/pysource.<configName>.sh
  <py_runcode>
#
hpc_runcode: cp HPC_STDIN ../;cd ..;sbatch < <hpc_stdin>
#
#cmd_obj:    mpif90  -c -convert big_endian -O2 -DHAVE_MPI -DHAVE_MED -DHAVE_VTK <mods> <incs> <f95name>
cmd_obj:    mpif90 -fPIC -c -convert big_endian -O2 -DHAVE_MPI -DHAVE_VTK <mods> <incs> <f95name>
cmd_lib:    mpif90 -fPIC -shared -convert big_endian -O2 -DHAVE_MPI -DHAVE_VTK -o <libname> <objs>
cmd_exe:    mpif90 -fPIC -o <exename> <objs> <libs>
#
cmd_obj_c: mpicc -fPIC -c <srcName> -o <objName>
#
libs_all: -L$MEDHOME/lib -lmed -L$HDF5HOME/lib -lhdf5
          -L$METISHOME/lib -lmetis
#
#
[eole.intel.debug]
brief: Debug mode Intel compiler 2017 with intel mpi 2017.0.998 in dynamic
#
sfx_lib:    .so
options:    mpi hpc
par_cmdexec: srun -n 1 -N 1 <config>/partel < PARTEL.PAR >> <partel.log>
mpi_cmdexec: mpirun -np <ncsize> <exename>
#
hpc_stdin: #!/bin/bash
  #SBATCH --job-name=<jobname>
  #SBATCH --output=<jobname>-<time>.out
  #SBATCH --error=<jobname>-<time>.err
  #SBATCH --time=<walltime>
  #SBATCH --ntasks=<ncsize>
  #SBATCH --partition=<queue>
  ##SBATCH --exclude=cn[0000-0000,0000]
  #SBATCH --exclusive
  #SBATCH --nodes=<ncnode>
  #SBATCH --ntasks-per-node=<nctile>
  source <root>/configs/pysource.<configName>.sh
  <py_runcode>
#
hpc_runcode: cp HPC_STDIN ../;cd ..;sbatch < <hpc_stdin>
#
cmd_obj:    mpif90 -fPIC -c -debug all -check all -traceback -ftrapuv -O0 -g -convert big_endian -DHAVE_MPI -DHAVE_VTK <mods> <incs> <f95name>
cmd_lib:    mpif90 -fPIC -shared -debug all -check all -traceback -ftrapuv -O0 -g -convert big_endian -O2 -DHAVE_MPI -DHAVE_VTK -o <libname> <objs>
cmd_exe:    mpif90 -fPIC -debug all -check all -traceback -ftrapuv -O0 -g -o <exename> <objs> <libs>
#
cmd_obj_c: mpicc -fPIC -c <srcName> -o <objName>
#
libs_all: -L$MEDHOME/lib -lmed -L$HDF5HOME/lib -lhdf5
          -L$METISHOME/lib -lmetis
#
# _____            _______________________________
# ____/ Windows 8 /______________________________/
#
[winHPC]
brief: Windows 8 with gfortran and mpich (from automatic installer)
#
cmd_obj:    gfortran -c -O2 -DHAVE_MPI -fconvert=big-endian -frecord-marker=4 <mods> <incs> <f95name>
cmd_lib:    ar cru <libname> <objs>
cmd_exe:    gfortran -fconvert=big-endian -frecord-marker=4 -lm -o <exename> <objs> <libs>
#
mpi_cmdexec:   mpiexec.exe -wdir <wdir> -n <ncsize> <exename>
#
incs_all:
#
sfx_lib:    .lib
sfx_exe:    .exe
#
incs_all:      -I <root>\..\mpich2\include
libs_all:      <root>\..\mpich2\lib\libfmpich2g.a
               <root>\..\metis\lib\libmetis.a

[winHPC.debug]
brief: Debug mode Windows 8 with gfortran and mpich (from automatic installer)
#
cmd_obj:    gfortran -c -Wall -g -O0  -fcheck=all -fbacktrace -ffpe-trap=invalid,zero,overflow -DHAVE_MPI -fconvert=big-endian -frecord-marker=4 <mods> <incs> <f95name>
cmd_lib:    ar cru <libname> <objs>
cmd_exe:    gfortran -g -fconvert=big-endian -frecord-marker=4 -lm -o <exename> <objs> <libs>
#
mpi_cmdexec:   mpiexec.exe -wdir <wdir> -n <ncsize> <exename>
#
incs_all:
#
sfx_lib:    .lib
sfx_exe:    .exe
#
incs_all:      -I <root>\..\mpich2\include
libs_all:      <root>\..\mpich2\lib\libfmpich2g.a
               <root>\..\metis\lib\libmetis.a
