# _____                              _______________________________
# ____/ TELEMAC Project Definitions /______________________________/
#
[Configurations]
configs:   C7.gfortran C7.pgi11 C7.ifort10 C7.gfortranHPC C7.ifort10HPC C7.nag5 C7.nag5HPC
# _____                        ____________________________________
# ____/ Generalr /___________________________________/
# Global declaration that are true for all the C7.* configuration
[general]
root:       /netdata/systel/V7P0
version:    v7p0
language:   2
modules:    system
options:    
#
sfx_zip:    .zip
sfx_lib:    .a
sfx_obj:    .o
sfx_mod:    .mod
sfx_exe:    
#
#val_root:   <root>/../validation
val_root:   <root>/examples
# also possible val_root:   <modpath>/calibration
val_rank:   all
#val_rank: =11
# also possible val_rank:   <3 >7 6
###
### To add MPI support
###
#
# options: parallel mpi
#
# cmd_obj += -DHAVE_MPI
#
# libs_all += <list-of-mpi-libraries>
#
# libs_partel += <path_to_metis>/lib/libmetis.a
#
###
### TO ADD MED SUPPORT
###
# cmd_obj += -DHAVE_MED
#
# incs_all += -I<path-to-med>/include
# libs_all += -L<path-to-hdf5>/lib -L<path-to-med>/lib -lmed -lhdf5 -lstdc++ -lz
###
### To add TECPLOT support (estel3d only)
###
# cmd_obj += -DHAVE_TECPLOT
#
# libs_all += <path-to-teplot>/tecplot.a
###
### TO ADD DREGEDIM SUPPORT
###
# cmd_obj += -DHAVE_DREDGESIM
#
#  !!! You will need to regenerate the cmdf files
#  !!! First remove all the cmdf 
#  !!! On linux run the following command from the root directory
#  !!! find sources/ -iname *.cmdf -exec rm -v {} +
#  !!! Then run compileTELEMAC.py --rescan
#
###
### TO ADD SCOTCH SUPPORT
###
#  !!! SCOTCH support still needs metis
# cmd_obj += -DHAVE_SCOTCH
# libs_partel += -L<path-to-scotch>/lib -lsctoch lsctocherr <path-to-metis>/lib/libmetis.a
#
###
### TO ADD PARMETIS SUPPORT
###
# cmd_obj += -DHAVE_PARMETIS
# libs_partel += <path-to-parmetis>/lib/libparmetis.a 
#
###
### TO ADD PTSCOTCH SUPPORT
###
#  !!! PTSCOTCH support still needs parmetis
# cmd_obj += -DHAVE_PARMETIS -DHAVE_PTSCOTCH
# libs_partel += -L<path-to-ptscotch>/lib -lptsctoch lptsctocherr <path-to-parmetis>/lib/libparmetis.a
#

# __                       	       _____________________________________
# ____/ Calibre7 GFORTRAN 4.4.5 Scalar /_______________________________/
[C7.gfortran]
#
cmd_obj:    gfortran -c -O3 -fconvert=big-endian -frecord-marker=4 -DHAVE_MED -DHAVE_DREDGESIM -DHAVE_VTK -DHAVE_TECPLOT <mods> <incs> <f95name>
cmd_lib:    ar cru <libname> <objs>
cmd_exe:    gfortran -fconvert=big-endian -frecord-marker=4 -o <exename> <objs> <libs>
cmd_exe_estel3d:    gfortran -fconvert=big-endian -frecord-marker=4 -o <exename> <objs> <libs> -lstdc++ -lz
#
mods_all:   -I <config>
libs_estel3d:/netdata/systel/LIBRARY/tecplot/tecplot_10/intel_64_10_C7/tecplot10.a
libs_all: -L/netdata/systel/LIBRARY/med-3.0.6/arch/C7/lib -lmed -L/netdata/systel/LIBRARY/hdf5-1.8.8/arch/C7/lib -lhdf5 -lstdc++ -lz
# _____               	              __________________________________
# ____/ Calibre7 GFORTRAN 4.4.5 MPICH _________________________________/
[C7.gfortranHPC]
#
options:    parallel mpi
#mpi_hosts: chap707 (example)
mpi_cmdexec: /usr/bin/mpirun -machinefile MPI_HOSTFILE -np <ncsize> <exename>
#
cmd_obj:    gfortran -c -O3 -fconvert=big-endian -frecord-marker=4 -DHAVE_MPI -DHAVE_MED -DHAVE_DREDGESIM -DHAVE_VTK -DHAVE_TECPLOT <mods> <incs> <f95name>
cmd_lib:    ar cru <libname> <objs>
cmd_exe:    /usr/bin/mpif90 -fconvert=big-endian -frecord-marker=4 -lpthread -lm -o <exename> <objs> <libs>
cmd_exe_estel3d:    /usr/bin/mpif90 -fconvert=big-endian -frecord-marker=4 -lpthread -lm -o <exename> <objs> <libs> -lstdc++ -lz
#
mods_all:   -I <config>
libs_estel3d:/netdata/systel/LIBRARY/tecplot/tecplot_10/intel_64_10_C7/tecplot10.a
#
#
incs_parallel:      -I /usr/include/mpi/
libs_partel:        /netdata/systel/LIBRARY/metis/gfortran_linux_C7/libmetis.a
libs_all:	    -pthread -I/usr/lib/openmpi/lib -L/usr/lib/openmpi/lib -lmpi_f90 
                    -lmpi_f77 -lmpi -lopen-rte -lopen-pal -ldl 
                    -Wl,--export-dynamic -lnsl -lutil -lm -ldl
                    -L/netdata/systel/LIBRARY/med-3.0.6/arch/C7/lib -lmed 
                    -L/netdata/systel/LIBRARY/hdf5-1.8.8/arch/C7/lib -lhdf5 -lstdc++ -lz

# _____                        ____________________________________
# ____/ Calibre7 Intel Scalar /___________________________________/
[C7.ifort10]
#
cmd_obj:    ifort -c -O3 -convert big_endian -DHAVE_VTK -DHAVE_DREDGESIM -DHAVE_MED -DHAVE_TECPLOT <mods> <incs> <f95name>
cmd_lib:    ar cru <libname> <objs>
cmd_exe:    ifort -convert big_endian -o <exename> <objs> <libs>
#
mods_all:   -I <config>
libs_estel3d:/netdata/systel/LIBRARY/tecplot/tecplot_10/intel_64_10_C7/tecplot10.a
libs_all: -L/netdata/systel/LIBRARY/med-3.0.6/arch/C7/lib -lmed -L/netdata/systel/LIBRARY/hdf5-1.8.8/arch/C7/lib -lhdf5 -lstdc++ -lz

# _____                     __________________________________
# ____/ Calibre7 INTEL MPICH _________________________________/
[C7.ifort10HPC]
#
options:    parallel mpi
#mpi_hosts: chap707 (example)
mpi_cmdexec: mpirun -machinefile MPI_HOSTFILE -np <ncsize> <exename>
#
cmd_obj:    ifort -c -O3 -convert big_endian -DHAVE_VTK -DHAVE_DREDGESIM -DHAVE_MED -DHAVE_TECPLOT -DHAVE_MPI <mods> <incs> <f95name>
cmd_lib:    ar cru <libname> <objs>
cmd_exe:    ifort -convert big_endian -o <exename> <objs> <libs>
cmd_exe_estel3d:    ifort -convert big_endian -o <exename> <objs> <libs> -lstdc++ -lz
#
mods_all:   -I <config>
libs_estel3d: /netdata/systel/LIBRARY/tecplot/tecplot_10/intel_64_10_C7/tecplot10.a
#
#
incs_parallel:      -I /netdata/systel/LIBRARY/mpi/intel_64_10/include
libs_partel:      /netdata/systel/LIBRARY/metis/intel_64_10_C7/libmetis.a
libs_all:         -L/netdata/systel/LIBRARY/mpi/intel_64_10/lib -lmpichf90 -lmpich -lopa 
                  -lpthread -lpthread -lrt
                  -L/netdata/systel/LIBRARY/med-3.0.6/arch/C7/lib -lmed 
                  -L/netdata/systel/LIBRARY/hdf5-1.8.8/arch/C7/lib -lhdf5 -lstdc++ -lz
# _____               	 __________________________________
# ____/ Calibre7 NAG 5.3 _________________________________/
[C7.nag5]
#
# no big Endian option... Please use the convert program in $HOMETEL/bin/convert
cmd_obj:    nagfor -c -O4 -w=obs -Oassumed -convert=BIG_ENDIAN -DHAVE_TECPLOT -DHAVE_MED -DHAVE_DREDGESIM <mods> <incs> <f95name>
cmd_lib:    ar cru <libname> <objs>
cmd_exe:    nagfor -o <exename> <objs> <libs>
cmd_exe_estel3d:    nagfor -o <exename> <objs> <libs> -lstdc++ -lz
#
mods_all:   -I <config>
libs_estel3d:/netdata/systel/LIBRARY/tecplot/tecplot_10/intel_64_10_C7/tecplot10.a
libs_all: -L/netdata/systel/LIBRARY/med-3.0.6/arch/C7/lib -lmed 
          -L/netdata/systel/LIBRARY/hdf5-1.8.8/arch/C7/lib -lhdf5 -lstdc++ -lz
#
# _____               	       __________________________________
# ____/ Calibre7 NAG 5.3 MPICH _________________________________/
[C7.nag5HPC]
#
options:    parallel mpi
#mpi_hosts: chap707 (example)
mpi_cmdexec: mpirun -machinefile MPI_HOSTFILE -np <ncsize> <exename>
#
cmd_obj:    nagfor -c -O4 -w=obs -Oassumed -convert=BIG_ENDIAN -DHAVE_TECPLOT -DHAVE_MED -DHAVE_DREDGESIM -DHAVE_MPI <mods> <incs> <f95name>
cmd_lib:    ar cru <libname> <objs>
cmd_exe:    nagfor -o <exename> <objs> <libs>
cmd_exe_esltel3d:    nagfor -o <exename> <objs> <libs> -lstdc++ -lz
#
mods_all:   -I <config>
libs_estel3d: /netdata/systel/LIBRARY/tecplot/tecplot_10/intel_64_10_C7/tecplot10.a
#
incs_parallel:      -I /netdata/systel/LIBRARY/mpi/NAG_64_5_C7/include
libs_partel:      /netdata/systel/LIBRARY/metis/gfortran_linux_C7/libmetis.a
libs_all:	    -L/netdata/systel/LIBRARY/mpi/NAG_64_5_C7/lib -lmpichf90 -lmpich -lopa -lpthread -lrt
                    -L/netdata/systel/LIBRARY/med-3.0.6/arch/C7/lib -lmed -L/netdata/systel/LIBRARY/hdf5-1.8.8/arch/C7/lib -lhdf5 -lstdc++ -lz
#
# _____                        ____________________________________
# ____/ Edf ivanoe Cluster  /_____________________________________/
[ivanoe.ifort12]
#
root:       /home/projets/systel/V7P0
version:    v7p0
language:   2
modules:    update system
#
options:    parallel mpi hpc
#mpi_hosts: chap707 (example)
par_cmdexec: srun -n 1 <config>/partel < PARTEL.PAR >> <partel.log>
mpi_cmdexec: mpiexec.hydra -machinefile MPI_HOSTFILE -np <ncsize> <exename>
#
hpc_stdin: #!/bin/bash
  #SBATCH --job-name=<jobname>
  #SBATCH --output=<jobname>-<time>.out
  #SBATCH --error=<jobname>-<time>.err
  #SBATCH --time=<walltime>
  #SBATCH --ntasks=<ncsize>
  #SBATCH --partition=<queue>
  ##SBATCH --exclude=cn[0000-0000,0000]
  ##SBATCH --exclusive
  srun hostname | sort > mpid.conf
  hosts=''
  for line in $(echo | cat mpid.conf)
  do
    hosts=$hosts:$line
  done 
  hosts=${hosts:1:${#hosts}}
  rm -f mpitasks mpitasks.conf mpid.conf
  runcode.py <codename> <casfiles> <options> --hosts=$hosts
#
hpc_cmdexec: sbatch < <hpc_stdin>
#
cmd_obj:    ifort  -c -O3 -convert big_endian -no-prec-div -O3 -sox -vec-report1 -xsse4.2 -DHAVE_MPI -DHAVE_MED -DHAVE_TECPLOT -DHAVE_DREDGESIM <mods> <incs> <f95name>
cmd_lib:    ar cru <libname> <objs>
cmd_exe:    ifort  -O3 -o <exename> <objs> <libs> 
#
mods_all:   -I <config>
#
incs_parallel:    -I /logiciels/intel/l_ics_2013.0.028/impi/4.1.0.024/intel64/include
incs_partel:      -I/home/projets/systel/opt/scotch-6.0.0/arch/ivanoe/include
libs_partel:      /home/projets/systel/opt/metis-5.1.0/arch/ivanoe/lib/libmetis.a
libs_all:         -L/logiciels/intel/l_ics_2013.0.028/impi/4.1.0.024/intel64/lib 
                  -L/home/projets/systel/opt/med-3.0.6/arch/ivanoe/lib -lmed
                  -L/home/projets/systel/opt/hdf5-1.8.8/arch/ivanoe/lib -lhdf5
                  -lmpi -lmpigf -lpthread -lz -lstdc++ -v
                   /home/projets/systel/LIBRARY/tecplot/tecplot_10/ivanoe_intel_openmpi_64_12/tecplot10.a
#
#
sfx_zip:    .zip
sfx_lib:    .a
sfx_obj:    .o
sfx_mod:    .mod
sfx_exe:    
#
val_root:   <root>/examples
val_rank:   all
# also possible val_rank:   <3 >7 6
# _____                        ____________________________________
# ____/ IBM BluegeneQ cluster /___________________________________/
[zumbrota.xlf14]
#
root:       /home/projets-bgq/systel/V7P0
version:    v7p0
language:   2
modules:    update system
#
options:    parallel mpi hpc
#
par_cmdexec: srun -n 1 <config>/partel < PARTEL.PAR >> <partel.log>
mpi_cmdexec: srun -n <ncsize> <exename>
#
hpc_stdin: #!/bin/bash
  #SBATCH --job-name=<jobname>
  #SBATCH --output=<jobname>-<time>.out
  #SBATCH --error=<jobname>-<time>.err
  #SBATCH --time=<walltime>
  #SBATCH --ntasks=<ncsize>
  #SBATCH --partition=<queue>
  ##SBATCH --exclude=cn[0000-0000,0000]
  ##SBATCH --exclusive
  srun hostname | sort > mpid.conf
  hosts=''
  for line in $(echo | cat mpid.conf)
  do
    hosts=$hosts:$line
  done 
  hosts=${hosts:1:${#hosts}}
  echo "$hosts" > test.txt
  rm -f mpitasks mpitasks.conf mpid.conf
  runcode.py <codename> <casfiles> <options> --hosts=$hosts
#
hpc_cmdexec: sbatch < <hpc_stdin>
#
cmd_obj:    mpixlf90_r -fc=bgxlf_r -c -O3 -big-endian -qnoescape -WF,-DHAVE_MPI -WF,-DHAVE_TECPLOT -WF,-DHAVE_DREDGESIM <mods> <incs> <f95name>
cmd_obj_dredgesim:    mpixlf90_r -fc=bgxlf_r -c -O0 -big-endian -qnoescape -WF,-DHAVE_MPI -WF,-DHAVE_TECPLOT -WF,-DHAVE_DREDGESIM <mods> <incs> <f95name>
cmd_lib:    ar cru <libname> <objs>
cmd_exe:    mpixlf90_r -o <exename> <objs> <libs>
#
mods_all:   -I <config>
#
incs_parallel:      -I /bgsys/drivers/ppcfloor/comm/xl/include
libs_partel:        /home/projets-bgq/systel/LIBRARY/metis-5.1.0/arch/zumbrota/lib/libmetis.a
libs_all:	    -L/bgsys/drivers/ppcfloor/comm/xl/lib -lmpichf90 -lmpich -lopa -lpthread -lrt 
                    -L/home/projets-bgq/systel/LIBRARY/med-3.0.6/arch/zumbrota/lib -lmed 
                    -L/home/projets-bgq/systel/LIBRARY/hdf5-1.8.8/arch/zumbrota/lib -lhdf5
                    /home/projets-bgq/systel/LIBRARY/tecplot/tecplot_10/BGQ_XLF_14/tecplot10.a
                    /bgsys/linux/RHEL6.3_V1R2M0-36/opt/ibmcmp/vacpp/bg/12.1/lib64/libibmc++.a
                    /bgsys/drivers/toolchain/V1R2M0-efix23/gnu-linux/powerpc64-bgq-linux/lib/libstdc++.a 
                    /home/projets-bgq/systel/LIBRARY/zlib-1.2.7/arch/zumbrota/lib/libz.a 
#
sfx_zip:    .zip
# .lib suffix not recognised by NAG
sfx_lib:    .a
sfx_obj:    .o
sfx_mod:    .mod
sfx_exe:    
#
#val_root:   <root>/../validation
val_root:   <root>/examples
# also possible val_root:   <modpath>/calibration
val_rank:   all
# also possible val_rank:   <3 >7 6
